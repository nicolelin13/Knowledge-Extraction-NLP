{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kunlin_CSC594-910Online_FinalProject_Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 0: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize, sent_tokenize, pos_tag\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from pandas import ExcelWriter\n",
    "from pandas import ExcelFile\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Read In Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_wikitext(text_oh):\n",
    "    #text = nltk.clean_html(text)\n",
    "    text_oh = BeautifulSoup(text_oh, 'html.parser')\n",
    "    text = text_oh.get_text()\n",
    "    text = re.sub(r'\\[\\[.*?:.*?\\]\\]', '', text)    # remove interwiki links\n",
    "    template_depth = 0\n",
    "    in_link = in_ext_link = False\n",
    "    link_words = []\n",
    "    pieces = []\n",
    "    for word in nltk.wordpunct_tokenize(text):\n",
    "        # skip everything in template braces\n",
    "        if '{{' in word:\n",
    "            template_depth += 1\n",
    "        elif '}}' in word:\n",
    "            template_depth -= 1\n",
    "        elif template_depth == 0:\n",
    "            # detect links\n",
    "            if '[[' in word:\n",
    "                in_link = True\n",
    "            elif ']]' in word:\n",
    "                # at the end of a link, output it as a named entity chunk\n",
    "                if link_words:\n",
    "                    pieces.append(nltk.Tree('NE', link_words))\n",
    "                    link_words = []\n",
    "                    in_link = False\n",
    "            elif '[' in word:\n",
    "                # start over if the link has | marking an alternate name\n",
    "                in_ext_link = True\n",
    "            elif in_ext_link and ']' in word:\n",
    "                in_ext_link = False\n",
    "            elif '|' in word and in_link:\n",
    "                link_words = []\n",
    "            else:\n",
    "                if in_link:\n",
    "                    link_words.append(word)\n",
    "                elif not in_ext_link:\n",
    "                    pieces.append(word)\n",
    "    return nltk.Tree('S', pieces)\n",
    "\n",
    "\n",
    "# FINAL USED TO READ IN THE TEXT DATA\n",
    "def readin_text_nohtml_notree(path, limit_n = 50000):\n",
    "    '''input: path for original text from the datset file\n",
    "    output: word tokenized sentences from all the files in one list\n",
    "    needed library: os, re, BeautifulSoup'''\n",
    "    files = list()\n",
    "    n = 0\n",
    "    for filename in os.listdir(path):\n",
    "        n += 1\n",
    "        if n < limit_n:\n",
    "            with open(os.path.join(path,filename), 'rU') as file:\n",
    "                text = file.read()\n",
    "                text_tree = chunk_wikitext(text)\n",
    "                text_notree = ' '.join(word for word in text_tree.leaves())\n",
    "                sentences = nltk.sent_tokenize(text_notree) \n",
    "                sentences = [nltk.word_tokenize(sent) for sent in sentences] \n",
    "                files += sentences\n",
    "        else: break\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Queena/anaconda/lib/python3.6/site-packages/ipykernel/__main__.py:51: DeprecationWarning: 'U' mode is deprecated\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 41s, sys: 2.24 s, total: 1min 43s\n",
      "Wall time: 1min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "path = 'articles/'\n",
    "demo_data = readin_text_nohtml_notree(path, limit_n=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Knowlege Extraction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grammar_VBN(sent): \n",
    "    '''dataset: lists of word tokenized sentences'''\n",
    "    \n",
    "    # Second part: used as verb, past participle (VBN)\n",
    "    # IBM <NP> is/are <VB> located <VBN> at <IN> Chicago <NE>\n",
    "    # IBM <NP>, <,> located <VBN> at <IN> Chicago <NE>, is a good company.\n",
    "    # IBM <NP>, <,> which <WDT> is/are located at Chicago\n",
    "    \n",
    "    # IBM, a Chicago based company, is good.\n",
    "    # Chicago <NE> based <VBN> company <NN>, <,> IBM <NP> is good.\n",
    "    # Located <VBN> in <IN> Chicago <NE>, <,> IBM <NP> is a good company.\n",
    "    \n",
    "    VBN_list = ['located','sited','placed','headquartered','positioned',\n",
    "                'stationed','situated','replaced',\n",
    "                'Located','Sited','Placed','Headquartered','Positioned',\n",
    "                'Stationed','Situated','Replaced']\n",
    "    VBN_list_tag = [(word,\"VBN\") for word in VBN_list]\n",
    "    verb_list_tag = [('is','VBZ'),('are','VBP')]\n",
    "    \n",
    "    VBN_grammar = r\"\"\"\n",
    "    NE: {<NNP|NNPS>+(<,><NNP|NNPS>)*(<IN><NNP|NNPS>)*}\n",
    "    NEP: {<DT>?(<JJ>*<NN>*<IN>)?<NE>}\n",
    "    NP1: {<JJ>*<CD>?<NN.*>+<POS>?<NN.*>*<CD>?}\n",
    "    NP2: {<''><''><NN.*><POS><''>}\n",
    "    VB: {<VBZ|VBP>}\n",
    "    CLAUSE: {<DT>?<NP1|NP2><,>?<WDT>?<VB>?<VBN><IN><NEP>}\n",
    "    {((<NEP><VBN><NN.*>)|(<VBN><IN><NEP>))<,>?<DT>?<NP1|NP2>}\n",
    "    \"\"\"\n",
    "    \n",
    "    cp = nltk.RegexpParser(VBN_grammar)\n",
    "    return_list = list()\n",
    "    X_Y = list()\n",
    "    leave2 = str()\n",
    "    leave3 = str()\n",
    "    if set(sent).intersection(set(VBN_list)):\n",
    "        tags = pos_tag(sent)\n",
    "        tree = cp.parse(tags)\n",
    "        for subtree in tree.subtrees():\n",
    "            if subtree.label() == 'CLAUSE': \n",
    "                leave = subtree.leaves()\n",
    "                if set(leave).intersection(set(VBN_list_tag)) and set(leave).intersection(set(verb_list_tag)):\n",
    "                    return_list.append(leave)\n",
    "                    break\n",
    "    return return_list\n",
    "\n",
    "\n",
    "def grammar_special_VBPZ(sent): \n",
    "    '''dataset: lists of word tokenized sentences'''\n",
    "    \n",
    "    # Fourth part: other special verbs (lie, sit) used as verb(VB & VBP & VBZ)\n",
    "    special_VBPZ_list = ['lie','sit','lies','sits']\n",
    "    special_VBPZ_list_tag = [('lie','VBP'),('sit','VBP'),\n",
    "                             ('lies','VBZ'),('sits','VBZ')]\n",
    "    special_VBPZ_grammar = r\"\"\"\n",
    "    NE: {<NNP|NNPS>+(<,><NNP|NNPS>)*(<IN><NNP|NNPS>)*}\n",
    "    NEP: {<DT>?(<JJ>*<NN>*<IN>)?<NE>}\n",
    "    NP1: {<JJ>*<CD>?<NN.*>+<POS>?<NN.*>*<CD>?}\n",
    "    NP2: {<''><''><NN.*><POS><''>}\n",
    "    VB: {<VBZ|VBP>}\n",
    "    CLAUSE: {<DT>?<NP1|NP2>(<,>?<WDT>?)<VB><IN><NEP>}\n",
    "    \"\"\"\n",
    "    cp = nltk.RegexpParser(special_VBPZ_grammar)\n",
    "    return_list = list()\n",
    "    \n",
    "    if set(sent).intersection(set(special_VBPZ_list)):\n",
    "        tags = pos_tag(sent)\n",
    "        tree = cp.parse(tags)\n",
    "        for subtree in tree.subtrees():\n",
    "            if subtree.label() == 'CLAUSE': \n",
    "                leave = subtree.leaves()\n",
    "                if set(leave).intersection(set(special_VBPZ_list_tag)):\n",
    "                    return_list.append(leave)\n",
    "                    break\n",
    "    return return_list\n",
    "    \n",
    "\n",
    "def grammar_special_VBG(sent): \n",
    "    '''dataset: lists of word tokenized sentences'''\n",
    "    \n",
    "    # Fifth part: other special verbs (lie, sit) used as verb, gerund(VBG)\n",
    "    special_VBG_list = ['lying','sitting','Lying','Sitting']\n",
    "    special_VBG_list_tag = [(word,'VBG') for word in special_VBG_list]\n",
    "    special_VBG_grammar = r\"\"\"\n",
    "    NE: {<NNP|NNPS>+(<,><NNP|NNPS>)*(<IN><NNP|NNPS>)*}\n",
    "    NEP: {<DT>?(<JJ>*<NN>*<IN>)?<NE>}\n",
    "    NP1: {<JJ>*<CD>?<NN.*>+<POS>?<NN.*>*<CD>?}\n",
    "    NP2: {<''><''><NN.*><POS><''>}\n",
    "    VB: {<VBZ|VBP>}\n",
    "    CLAUSE: {<DT>?<NP1|NP2><,>?(<WDT><VB>)?<VBG><IN><NEP>}\n",
    "    {<VBG><IN><NEP><,><DT>?<NP1|NP2>}\n",
    "    \"\"\"\n",
    "\n",
    "    cp = nltk.RegexpParser(special_VBG_grammar)\n",
    "    return_list = list()\n",
    "    if set(sent).intersection(set(special_VBG_list)):\n",
    "        tags = pos_tag(sent)\n",
    "        tree = cp.parse(tags)\n",
    "        for subtree in tree.subtrees():\n",
    "            if subtree.label() == 'CLAUSE': \n",
    "                leave = subtree.leaves()\n",
    "                if set(leave).intersection(set(special_VBG_list_tag)):\n",
    "                    return_list.append(leave)\n",
    "                    break\n",
    "    return return_list\n",
    "\n",
    "\n",
    "def location_extract(dataset):\n",
    "    return_list = list()\n",
    "    \n",
    "    for sent in dataset:\n",
    "        list1 = grammar_VBN(sent)\n",
    "        list2 = grammar_special_VBPZ(sent)\n",
    "        list3 = grammar_special_VBG(sent)\n",
    "        list_totle = [list1, list2, list3]\n",
    "        for sublist in list_totle:\n",
    "            if sublist:\n",
    "                return_list += sublist\n",
    "    return return_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 34s, sys: 2.55 s, total: 3min 37s\n",
      "Wall time: 4min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sample_demo = location_extract(demo_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('This', 'DT'),\n",
       " ('tunnel', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('located', 'VBN'),\n",
       " ('at', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('Great', 'NNP'),\n",
       " ('Dividing', 'NNP'),\n",
       " ('Range', 'NNP')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_demo[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Sentence':sample_demo})\n",
    " \n",
    "writer = ExcelWriter('Sample_demo.xlsx')\n",
    "df.to_excel(writer,'Sheet1',index=False)\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
